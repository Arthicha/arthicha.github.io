<!DOCTYPE html>
<html>
<head>  
    <title>Arthicha Srsuchinnawong</title>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-07XCKQCFKP"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-07XCKQCFKP');
    </script>
</head>  



<style>
* {
   margin: 0px;
   padding: 0px;
}


.button {
  background-color: #2E9CCA; /* Green */
  border: none;
  color: white;
  padding: 0.1vw 0.1vw;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 0.8vw;
  margin: 0.1vw 0.1vw;
  cursor: pointer;
}

.icon-bar {
  width: 100%;
  background-color: #AAABB8;
  overflow: auto;
}

.icon-bar a {
  float: left;
  width: 16%;
  text-align: center;
  padding: 1.5vw 0;
  transition: all 0.3s ease;
  color: white;
  font-size: 1.5vw;
  border-left:2px solid white;
}

.icon-bar a:hover {
  background-color: #29648A;
}

.active {
  background-color: #AAABB8; /* Green */
  color = Navy
}

.wrapper {
  display: inline-flex;
  justify-content: space-between;
  padding: 0 0 0 0;
  margin: 0 0 0 0;
}

.smallcol {
    width:19%;
    height:auto;
    float: left;
   background-color:white;
   position: -webkit-sticky;
  position: sticky;
  top: 0;
 }

.maincol {
    /*width:74%;*/
    width:79vw;
    height:auto;
    float: left;
   background-color:white;
   border-right: 1px solid black;
   line-height: 0vw;
   overflow: auto;
   padding: 0 0 0 0;
    margin: 0 0 0 0;
 }

p.paragraph {
  word-wrap: break-word;
  text-align: justify;
  text-justify: inter-word;
  font-size:1.2vw;color: black;margin-bottom:1.0vw;margin-left:0vw;margin-top:0.5vw;margin-right:2vw

}


.projectimg {
  display: inline-block;
  opacity: 0.3;
  /*width:24.3vw;*/
  padding: 0 0 0 0;
  margin: 0 0 0 0;
  float: center;
  line-height: 0vw;
  transition: all 0.3s ease;
  filter: blur(0.1vw);
  font-size: 0vw;
  cursor: pointer;
  border: none;
  position: relative;
  top:0vw;left: 0vw;
  width:27vw;height:20vw
}


.projectimg:hover {opacity:0.6;filter: blur(0.0vw)}

div.information{
  display:none;
  background-color: white;
  width:100%;
}

p.information {
  word-wrap: break-word;
  text-align: justify;
  text-justify: inter-word;
  font-size:1.2vw;
  color: black;
  margin-bottom:1.0vw;margin-left:2vw;margin-top:0.5vw;margin-right:2vw;
  background-color:white;
  line-height: normal;
}

p.heading {

  word-wrap: break-word;
  text-align: justify;
  text-justify: inter-word;
  font-size:1.8vw;
  color: #29648A;
  margin-bottom:1.0vw;margin-left:2vw;margin-top:1.0vw;margin-right:2vw;
  background-color:white;
  line-height: normal;
}

b.subheading {

  word-wrap: break-word;
  text-align: justify;
  text-justify: inter-word;
  font-size:1.3vw;
  margin-bottom:1.0vw;margin-left:2vw;margin-top:1.0vw;margin-right:2vw;
  background-color:white;
  line-height: normal;
}

.projectname {
  position: absolute;
  z-index: 0;
  font-size: 1.8vw;
  transform: translate(-50%,-50%);
  color: white;
  line-height: 1.8vw;
  text-align: center;
  vertical-align: sub;
}

#slider {
        width: 600px;
        height: 300px;
        background-image: url("img/slide0.jpg");
        background-size: cover;
    }

.caption {
  margin-top: 1.1vw;
  margin-bottom: 1.1vw;
  font-size:1.1vw;
  font-style: italic;
}

i.year {
  font-size:1.2vw;
  color: #29648A;
  font-weight: bolder;
}

.fadedblack {
  background: rgba(0,0,0,0.8);float:center;padding: 0 0 0 0; margin: 0 0 0 0;display:flex;
}

img.res {
  width: 50%;
  height: auto;
}

video.res {
  width: 50%;
  height: auto;
}

iframe.res {
  width: 30vw;
  height: 20vw;
}

</style>

<script>
var projw = 20; 
var projh = 30;
</script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<div class="icon-bar">
  <a class="active" href="main.html" style="text-decoration: none"> <b>HOME</b> </a>
  <a href="education.html" style="text-decoration: none"> <b>EDUCATION</b></a> 
  <a href="experience.html" style="text-decoration: none"> <b>EXPERIENCES</b></a> 
  <a href="publication.html" style="text-decoration: none"> <b>PUBLICATIONS</b></a> 
  <a href="project.html" style="text-decoration: none"> <b>PROJECTS</b></a> 
  <a href="#" style="text-decoration: none"> <b>NEWS</b></a> 
</div>
<img src="pictures/cover/cover0.png" id="slider" style="display:block;height:30vw;width:100%;opacity:75%;"> </img>
<script>

    var images = new Array ("cover0.png","cover1.png")
    var image_count = Math.floor(Math.random() * 2);

    function rollover (image_id, millisecs) {
      var image = document.getElementById(image_id);
      image.src = "pictures/cover/"+images[image_count];
      image_count++;
      if (image_count >= images.length) {
        image_count = 0;
      }
      setTimeout("rollover('" + image_id + "'," + millisecs + ");",millisecs);
    }

    rollover("slider",10000);

</script>
 
 <div style="background-color: #464866;width: 100%;overflow: auto;height=4mm"> 
  <p style="color:#464866;font-size:0.5vw"> Hi</p>
 </div> 

<div class="wrapper"> <!------------------------ page ------------------------------------->
  <div class="maincol"> <!------------------------ main column ------------------------------------->
    
    <!------------------------ heading ------------------------------------->
    <br><div> <b><span style="color: #29648A;font-size:2.4vw;margin-left:2vw;line-height: 4.8vw;">Projects</span></b> </div><br>

    <!------------------------ overall description ------------------------------------->
    <p class="paragraph" style="margin-left:2vw;margin-bottom: 2vw"> These are some of my projects. Click the images for more details. </p> 


    <div class="fadedblack"> <!------------------------ projects row 1 ------------------------------------->
      <h2 class="projectname" style="top:52vw;left: 14.265vw;" id="name1"> Long-term autOnomy <br> For service robots <br> in consTruction </h2>
      <img class = "projectimg" style= "top:0vw;left: 0vw;" id="project1" src="pictures/project/loft_cover.png" onclick="clicked(1)" onmouseover="hover(1)" onmouseout="over(1)"> </img>

      <h2 class="projectname" style="top:52vw;left: 40.795vw;" id="name2"> Force/Time-based <br> Variable Impedance <br> Learning </h2>
      <img class="projectimg" style= "top:0vw;left: 0%;" id="project2" src="pictures/project/learningfromdemonstration_cover.png" onclick="clicked(2)" onmouseover="hover(2)" onmouseout="over(2)" > </img>
      
      <h2 class="projectname" style="top:52vw;left: 67.325vw;" id="name3"> NeuroVis </h2>
      <img class="projectimg" style= "top:0vw;left: 0%;" id="project3" src="pictures/project/neurovis_cover.png" onclick="clicked(3)" onmouseover="hover(3)" onmouseout="over(3)" >  </img>
    </div> <!------------------------ <end> projects row 1 ------------------------------------->
    

      <!------------------------ information row 1/1 ------------------------------------->
    <div id="info1" class="information" style="display:none;"> 
      <p class="heading"> <b> LOFT: Long-term autOnomy For service robots in consTruction </b> <i class="year"> (2022) </i></p>
      <p class="information"> 
        LOFT’s purpose is to explore the potential of robotics and AI in the construction sector by employing mobile service robots with long-term autonomy to transport material for workers reliably and efficiently while maintaining safe worker-robot interaction.  To achieve the aim, LOFT will develop and deliver intelligent robot control technology with advanced perception and safe and social-aware navigation for service robots in the construction industry.
      </p>

      <figure>
        <center>
        <img src="pictures/project/loft_corridor1.jpg" class='res' style="width:50%">
        <p class="caption">Figure: simulated Capra Hircus robot.</p>
        </center>
      </figure>

      <figure>
        <center>
        <img src="pictures/project/loft_corridor2.jpg" class='res' style="width:30%">
        <img src="pictures/project/loft_corridor3.jpg" class='res' style="width:30%">
        <p class="caption">Figure: simulation.</p>
        </center>
      </figure>

      <p class="information"> 
        <b> Project website: </b> <a href="https://ens-lab.sdu.dk/loft-project/" style="text-decoration: none; color:blue;font-size:1.2vw"> https://ens-lab.sdu.dk/loft-project/</a>
      </p>

    </div>
    <!------------------------ <end> information row 1/1 ------------------------------------->


   <!------------------------ information row 1/1 ------------------------------------->
    <div id="info2" class="information" style="display:none;"> 
      <p class="heading"> <b> Force/Time-based Variable Impedance Learning  </b> <i class="year"> (2022) </i></p>
      <p class="information"> 
        <figure>
        <center>
        <img src="pictures/project/learningfromdemonstration_overview.PNG" class='res' style="width:70%">
        <p class="caption">Figure: overview.</p>
        </center>
      </figure>

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/sSCtei65aYQ" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: demonstration.</p>
        </center>

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/WKuQOUfGybg" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: admittance control.</p>
        </center>

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/OjwU0gt8uMA" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: execution.</p>
        </center>

      </p>

      <p class="information"> 
        Modify and reproduce: Abu-Dakka, Fares J., Leonel Rozo, and Darwin G. Caldwell. "Force-based variable impedance learning for robotic manipulation." Robotics and Autonomous Systems 109 (2018): 156-167, <a href="https://www.sciencedirect.com/science/article/pii/S0921889018300125" style="text-decoration: none; color:blue;font-size:1.2vw"> https://doi.org/10.1016/j.robot.2018.07.008 </a>.
      </p>

      
    </div>
    <!------------------------ <end> information row 1/1 ------------------------------------->

    <!------------------------ information row 1/2 ------------------------------------->
    <div id="info3" class="information" style="display:none;"> 
      <p class="heading"> <b> NeuroVis - neuro visualization <i class="year"> (2020-2021) </i> </b></p>
      <p class="information"> 
        Understanding the real-time dynamical mechanisms of neural systems remains a significant issue, preventing the development of efficient neural technology and user trust. This is because the mechanisms, involving various neural spatial-temporal ingredients [i.e., neural structure (NS), neural dynamics (ND), neural plasticity (NP), and neural memory (NM)], are too complex to interpret and analyze altogether. While advanced tools have been developed using explainable artificial intelligence (XAI), node-link diagram, topography map, and other visualization techniques, they still fail to monitor and visualize all of these neural ingredients online. Accordingly, we propose here for the first time “NeuroVis,” real-time neural spatial-temporal information measurement and visualization, as a method/tool to measure temporal neural activities and their propagation throughout the network. By using this neural information along with the connection strength and plasticity, NeuroVis can visualize the NS, ND, NM, and NP via i) spatial 2D position and connection, ii) temporal color gradient, iii) connection thickness, and iv) temporal luminous intensity and change of connection thickness, respectively. This study presents three use cases of NeuroVis to evaluate its performance: i) function approximation using a modular neural network with recurrent and feedforward topologies together with supervised learning, ii) robot locomotion control and learning using the same modular network with reinforcement learning, and iii) robot locomotion control and adaptation using another larger-scale adaptive modular neural network. The use cases demonstrate how NeuroVis tracks and analyzes all neural ingredients of various (embodied) neural systems in real-time under the robot operating system (ROS) framework. To this end, it will offer the opportunity to better understand embodied dynamic neural information processes, boost efficient neural technology development, and enhance user trust.
        </p>
      

      <b class="subheading"> Use Case I: Hexapod Robot Locomotion </b>

      <figure>
        <center>
        <img src="pictures/project/neurovis_usecase1.PNG" class='res' style="width:50%">
        <p class="caption">Figure: use case I: hexapod robot locomotion.</p>
        </center>
        </figure>

      <p class="information"> 
        Neural visualization from NeuroVis showing the overview of the NS of the adaptive neural control network (A). This neural visualization is created based on information published through ROS messages from the adaptive neural control. The ND is presented by the neurons C0, C1, DL0-DL11, CF0-CF5, and FT0-FT5 which are highlighted in blue. The NP is presented by the neurons d0-d11, which are highlighted in red. The NM is represented by the neurons S0-K0 to S5-K5, which are highlighted in green. (B) The diagram of the adaptive neural control with the highlighted neural components that are employed to present the ND, NP, and NM. (C)Simulated robot. (D) Physical robot (Thor and Manoonpong, 2019).

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/-GHfUVBz1ek" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: use case I: hexapod robot locomotion.</p>
        </center>
      </p>

      <b class="subheading"> Use Case II: Hexapod Locomotion Learning </b>

      <figure>
        <center>
        <img src="pictures/project/neurovis_usecase2.PNG" class='res' style="width:50%">
        <p class="caption">Figure: use case II: robot locomotion learning.</p>
        </center>
        </figure>

      <p class="information"> 
        Neural visualization from NeuroVis showing the use of NeuroVis for robot locomotion control and reinforcement learning in a closed-loop embodied neural network. (A) Neural visualization of NeuroVis. (B) Neural control that is used in the second use case and visualized in (A). (C) Simulated and physical robots. (D) Learned optimal output signal for optimal swing and stance patterns. (E) Example of the reward signal for learning the connection weights between K1-8 and O to obtain the optimal output signal.

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/D29m6il3Mp0" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: use case II: robot locomotion learning.</p>
        </center>
      </p>

      


      <p class="information">
      <br> 
        <b>Reference:</b>  <p class="information">
          Arthicha Srisuchinnawong, Jettanan Homchanthanakul, and Poramate Manoonpong (2021) <b> NeuroVis: Real-time Neural Information Measurement and Visualization of Embodied Neural Systems</b>, Front. Neural Circuits. <a href="https://www.frontiersin.org/articles/10.3389/fncir.2021.743101/full" style="text-decoration: none; color:blue;font-size:1.2vw"> DOI: 10.3389/fncir.2021.743101</a>. </p>
      </p>



    </div>
    <!------------------------ <end> information row 1/2 ------------------------------------->

    

    <div class="fadedblack"> <!------------------------ projects row 2 ------------------------------------->
      <h2 class="projectname" style="top:72vw;left: 14.265vw;" id="name4"> Neural Control for <br> Gecko Robots </h2>
      <img class="projectimg" id="project4" src="pictures/project/geckorobot_cover.png" onclick="clicked(4)" onmouseover="hover(4)" onmouseout="over(4)"> 

      <h2 class="projectname" style="top:72vw;left: 40.795vw;" id="name5"> Robot Arm for <br> Advanced <br> Pick-and-Place </h2>
      <img class = "projectimg" id="project5" src="pictures/project/module89_cover.png" onclick="clicked(5)" onmouseover="hover(5)" onmouseout="over(5)"> </img>

      <h2 class="projectname" style="top:72vw;left: 67.325vw;" id="name6"> Drawing Bot </h2>
      <img class="projectimg" id="project6" src="pictures/project/drawingbot_cover.png" onclick="clicked(6)" onmouseover="hover(6)" onmouseout="over(6)"> 
    </div>
    <!------------------------ <end> projects row 2 ------------------------------------->

      <!------------------------ information row 1/3 ------------------------------------->
    <div id="info4" class="information" style="display:none;"> 
      <p class="heading"> <b> Neural Control for Gecko Robots </b> <i class="year"> (2018-2020) </i> </p>
      <p class="information"> 
        Gecko is an interesting animal that is well-known for its ability to climb walls even without its tail. People have taken gecko as inspiration for many things including climbing robots. In 2018, I got introduced to an adhesive material, which I call gecko tape, and I installed it at the robot feet. From 2018 to 2020, my colleagues and I have developed three versions of tailless gecko robots and the neural control for each to study locomotion with gait adaptation, obstacle avoidance, and leg posture adaptation. 
      </p> 

      <b class="subheading"> Locomotion with Gait Adaptation </b>
      <p class="information"> 
        A basic neural locomotion control for generating different gaits was implemented on the gecko robot. From the preliminary experiment, each gait had its characteristics. For example, the trot gait is fast, but the robot with the trot gait always slips and falls on steep slopes. The wave gait, on the other hand, is slow but is more stable. Therefore, we used the pitch angle as the feedback to determine the robot's gait and proposed the gecko robot with gait adaptation. With this strategy, the robot can translate to a suitable gait autonomously by reducing the speed to gain more stability when climbing steep slopes. Details of this work is in <a href="https://ieeexplore.ieee.org/abstract/document/8981580" style="text-decoration: none;">the paper</a>.
        <center>
          <video class="res" allow="autoplay; encrypted-media;" frameborder="0" controls loop autoplay>
            <source src="http://www.manoonpong.com/ICAR2019/video.mp4" type="video/mp4">
          </video>
          <p class="caption">Video: The gecko robot climbing with adaptable gait a changing slope.</p>
          </center>
        </p>

      <b class="subheading"> Locomotion on Vertical Surfaces and Obstacle Avoidance </b>
      <p class="information"> 
        Extending from our previous work, my colleagues and I designed a new gecko robot and developed more advanced neural locomotion control. In 2020, we have presented the first tailless gecko robot that is able to climb vertical surfaces and avoid obstacles by climbing sideways. Up to that time, climbing a vertical surface had not been reported by any tailless gecko robots; additionally, the previously developed gecko robots could only climb forward.  
        <center>
        <video class="res" allow="autoplay; encrypted-media;" frameborder="0" controls loop autoplay>
        <source src="http://www.manoonpong.com/JINT/video1.mp4" type="video/mp4">
        </video>
        <p class="caption">Video: The gecko robot with adaptable gait climbing a changing slope upto 90 degree.</p>
        <video class="res" allow="autoplay; encrypted-media;" frameborder="0" controls loop autoplay>
        <source src="http://www.manoonpong.com/JINT/video2.mp4" type="video/mp4">
        </video>
        <p class="caption">Video: The gecko robot climbing sideways to avoid the obstacles locating on its path.</p>
        </center>
      </p>
    </div>
    <!------------------------ <end> information row 1/3 ------------------------------------->

    
    <!------------------------ infomation row 2/1 ------------------------------------->
    <div id="info5" class="information" style="display:none;"> 
      <p class="heading"> <b> Robot Arm for Advanced Pick-and-Place </b> <i class="year"> (2018) </i> </p>
      <p class="information"> 
        With a team of ten, we built an intelligent robot arm integrated with vision-based machine learning for advanced pick-and-place. The robot's tasks are twofold. First, the robot detects nameplates locating on the sidewalls and floor, and it picks them. Next, it attaches all the plates to the front wall in the correct order according to the labels.
        <center>
        <figure>
        <img src="pictures/project/module89_team.png" class="res">
        <p class="caption">Figure: The robot arm for advanced pick-and-place.</p>
        </figure>
        </center>
      </p>
      
      <b class="subheading"> Hardware </b>
      <p class="information"> 
        The robot is a 6-DOF articulated robot arm with a pneumatic gripper attached to its spherical wrist. It was created from scratch with a budget of 30,000 THB. The high-level controller is a computer, while the low-level PID controller is an STM32 microcontroller. 
      </p>

      <b class="subheading"> Robot Control </b>
      <p class="information"> 
        Due to the non-standard robot configuration, we had to solve inverse kinamatics by ourselves, and we did this via geometry approach and kinematic decoupling. The simulation used to validate the inverse kinamatic equation is shown below. Another challange in robot control is path planing since the robot should avoid hitting the walls and itself. Consequently, we employed pottential field as our path planing algorithm. 
        <center>
        <iframe class="res" src="https://www.youtube.com/embed/71FU94bBjBs?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=71FU94bBjBs&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: The simulation used to validate the inverse kinamatic equation.</p>
        </center>
      </p>

      <b class="subheading"> Robot Vision </b>
      <p class="information"> 

        The robot interprets the labels by using an ensemble machine learning, where the probabilistic output vectors of three models (convolutional neural network (CNN), random forest (RF), and k-nearest neighbor (KNN)) are averaged. Dealing with data hungry deep learning, the models were trained on an enormous pool of synthetic data generated from font scraping. However, the performance of the model on captured labels is not good without applying an edge detector and skeletonizaition to all the images, as presented in the table below (more details can be seen from <a href="https://ieeexplore.ieee.org/abstract/document/8712759" style="text-decoration: none;"> the paper</a>). At last, the classification is robust enough to correctly classify the labels with complex fonts and the handwritten ones as shown in the video below. 
        <center>
        <figure>
        <img src="pictures/project/module89_transfer.PNG" class='res' style="width:85%">
        <p class="caption">Table: The F1-score of the classification.</p>
        </figure>
        <iframe class='res'src="https://www.youtube.com/embed/8SWfL4L6MHo?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=8SWfL4L6MHo&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: The demonstration of the classification.</p>
        </center>
      </p>
      
      <b class="subheading"> Demonstration </b>
      <p class="information" style="margin-bottom: 3vw"> </p>
      <p class="information"> 
        <center>
          <iframe class='res' src="https://www.youtube.com/embed/y-yY0PuGL6Y?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=y-yY0PuGL6Y&loop=1" allow="autoplay; encrypted-media;" frameborder="0"> </iframe>
          <p class="caption">Video: Demonstration.</p>
          <iframe class='res' src="https://www.youtube.com/embed/lX-O7UcXecY?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=lX-O7UcXecY&loop=1" allow="autoplay; encrypted-media;" frameborder="0"> </iframe>
          <p class="caption">Video: Demonstration.</p>
        </center>
      </p>

      <p class="information" style="margin-bottom: 3vw"> </p>
    </div>
    <!------------------------ <end> infomation row 2/1 ------------------------------------->


    <!------------------------ infomation row 2/2 ------------------------------------->
    <div id="info6" class="information" style="display:none;"> 
      <p class="heading"> <b> Drawing Bot </b> <i class="year"> (2017) </i>  </p>
      <p class="information"> 
        Drawing Bot was built by me and my team in 2017 with our expertise in mechanical design, embedded system, control theory, image processing, and machine learning. The robot's goal is to copy, sketch, and paint the picture shown beforehand. If the picture contains more than one color, it is drawn with the corresponding color pens. In December 2017, Drawing Bot was exhibited in the International Robotics Exhibitions at the Thailand Robotics Week 2017.

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/7Bu3LbkdVA8?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=7Bu3LbkdVA8&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: Drawing Bot drawing a picture of me and my team.</p>
        </center>

      </p>

      <b class="subheading"> Hardware </b>
      <p class="information"> 
        Drawing Bot is a cartesian robot. The horizontal movement is controlled by two sets of a lead screw and a DC motor with an encoder. The vertical movement is controlled by another DC motor, limit switches, and a rack, to which a 3d-printed gripper is mounted. Inspired by hand, the gripper contains a servo motor connecting to two four-bar linkage mechanisms: the robot's thumb, holding the pen along the lateral direction, and the robot's index, holding the pen along the front. In total, the robot costs 15,000 THB.

        <center>
          <figure>
            <img src="pictures/project/drawingbot_structure.PNG" class='res'>
            <p class="caption">Figure: Drawing Bot.</p>
          </figure>
        </center>
      </p>

      <b class="subheading"> Low-level control </b>
      <p class="information"> 
        An embedded controller based on a PIC24 chip is employed. It serves as the PID controller, handling under the interrupt service routine. Additionally, the anti-windup technique is implemented to deal with control input saturation. The target and the state of the robot are from a PC acting as the high-level controller through wired communication.  
      </p>

      <b class="subheading"> High-level control </b>
      <p class="information"> 
        After the image is captured, K-mean clustering divides the pixels into three groups, the centroids of which are compared with the pen color to deal with multi-color images. The algorithm subsequently extracts lines from contours, edges, and shading regions before the lines are combined to the TCP trajectory of image coordinates. A polynomial regression model, trained with a calibration sheet, then converts the trajectory into world coordinates; and they are encoded to high-level protocol later on.
        <center>
          <figure>
            <img src="pictures/project/drawingbot_trajectory.png" class='res' style="width:25%">
            <p class="caption">Figure: Visualization of the trajectory.</p>
          </figure>
        </center>
      </p>
      <b class="subheading"> Results </b>

      <p class="information" style="margin-bottom: 3vw"> </p>
      <p class="information"> 
        <center>
          <figure>
            <img src="pictures/project/drawingbot_result1.png" class='res' style="width:25%">
            <img src="pictures/project/drawingbot_result2.png" class='res' style="width:25%">
            <p class="caption">Figure: Drawing Bot's paintings: Kyro Ren's helmet (left) and Stormtrooper (right).</p>
          </figure>
          <figure>
            <img src="pictures/project/drawingbot_cover.png" class='res'>
            <p class="caption">Figure: Drawing Bot's paintings.</p>
          </figure>
        </center>
      </p>
    </div>
    <!------------------------ <end> infomation row 2/2 ------------------------------------->

    


    <!------------------------ projects row 3 ------------------------------------->


    <div class="fadedblack">
      <h2 class="projectname" style="top:92vw;left: 14.265vw;" id="name7"> Ravana <br> the Animatronic Robot </h2>
      <img class = "projectimg" id="project7" src="pictures/project/ravana_cover.png" onclick="clicked(7)" onmouseover="hover(7)" onmouseout="over(7)"> </img>

      <h2 class="projectname" style="top:92vw;left: 40.795vw;" id="name8"> Model-Based <br> Reinforcement Learning </h2>
      <img class="projectimg" id="project8" src="pictures/project/modelbased_cover.png" onclick="clicked(8)" onmouseover="hover(8)" onmouseout="over(8)"> 

      <h2 class="projectname" style="top:92vw;left: 67.325vw;" id="name9"> Learning Locomotion </h2>
      <img class="projectimg" id="project9" src="pictures/project/rbppo_cover.png" onclick="clicked(9)" onmouseover="hover(9)" onmouseout="over(9)"> 
    </div> 
    <!------------------------ <end> projects row 3 -------------------------------------> 


    <!------------------------ infomation row 2/3 ------------------------------------->
    <div id="info7" class="information" style="display:none;"> 
      <p class="heading"> <b> Ravana - the Animatronic Robot </b> <i class="year"> (2016) </i></p>
      <p class="information"> 
        coming soon . . .
      </p> 
    </div>
    <!------------------------ <end> infomation row 2/3 ------------------------------------->

   <!------------------------ information row 3/1 ------------------------------------->
   <div id="info8" class="information" style="display:none;"> 
      
      <p class="heading"> <b> Model-Based Reinforcement Learning </b> <i class="year"> (2019) </i> </p>
      <p class="information"> 
        Model-free reinforcement learning (RL) is a popular learning technique in robotics; however, it requires a long learning time. <a href="https://arxiv.org/abs/1708.02596" style="text-decoration: none;"> Model-based RL</a> is an alternative method to accelerate and constrain learning. With this method, the internal model of the system is learned along with the sequence of actions is optimized for a high reward.
        <center>
        <figure>
            <img src="pictures/project/modelbased_overview.PNG" class='res'>
          </figure>
        <p class="caption">Figure: Overview of the model-based reinforcement learning used here.</p>
        </center>
      </p>

      <p class="information"> 
        Instead of using a neural network to approximate the system dynamics, the tensor equations are derived based on the fourth-order <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods" style="text-decoration: none;"> Runge-Kutta method (RK4)</a> to speed up the process further. A constrained optimization, named <a href="https://en.wikipedia.org/wiki/Sequential_quadratic_programming" style="text-decoration: none;"> sequential quadratic programming (SQP)</a>, subsequentially optimizes the actions/control inputs for the highest reward in a given time horizon. Throughout the process, the parameters of the tensor equations are adapted using a gradient-based approach such that the internal dynamic model matches the system dynamics. 
      </p>
      <b class="subheading"> Results </b>
      <p class="information"> 
        Three dynamical systems were employed for the study: an inverted pendulum, an acrobot, and a compass gait. The inverted pendulum has one active sliding joint (the cart), and the goal is to swing the pole (green ball) to the upright position. The Acrobot also share the same objective, but the active joint is the revolute joint between two links. Finally, the task of the compass gait is to walk forward as fast as it can without falling down. 
        </p> 

      <p class="information"> 
        
        <center>
        <iframe class='res' src="https://www.youtube.com/embed/RLEyvARsUbc?start=0&;end=7&;rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=RLEyvARsUbc&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: The inverted pendulum.</p>
        </center>

        <center>
        <iframe class='res' src="https://www.youtube.com/embed/7LDrV0sKHgk?start=0&;end=7&;rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=7LDrV0sKHgk&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: The Acrobot.</p>
        </center>


        <center>
        <iframe class='res' src="https://www.youtube.com/embed/L1HZXZxPtp4?start=2&;end=10&;rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=L1HZXZxPtp4&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: The compass gait.</p>
        </center>
      </p> 
      <p class="information" style="margin-bottom: 3vw"> </p>
    </div>
    <!------------------------ <end> information row 3/1 ------------------------------------->


    <!------------------------ information row 3/2 ------------------------------------->
    <div id="info9" class="information" style="display:none;"> 
      <p class="heading"> <b> Learning Locomotion </b>  <i class="year"> (2019) </i> </p>

      <p class="information"> 
        Dynamic models of multi-joint legged robots are difficult to obtain, so they are hard to control as well. Reinforcement learning (RL) allows training a neural network that maps observations directly to actions and produces successful locomotion.
        <center>
        <iframe class='res' src="https://www.youtube.com/embed/SOISIxcmLSY?rel=0&;controls=0&;showinfo=0;autoplay=1&mute=1;&playlist=SOISIxcmLSY&loop=1" allow="autoplay; encrypted-media;" frameborder="0"></iframe>
        <p class="caption">Video: Learning locomotion.</p>
        </center>
      </p>

      <p class="information"> 
        Proximal Policy Optimization (PPO) is one RL method that commonly leads to good results with a high reward. One variance of PPO utilizes KL divergence to limit the change of the network parameters and prevent falling-off-a-cliff. However, some sets of the hyperparameters may result in being stuck at local optima, as shown in the figure below. When this happens, the robot receives constant rewards and believes that it has selected the optimal actions, which actually is not. 
        <center>
        <figure><img src="pictures/project/rbppo_local1.gif" class='res' style="width:25%">
          <img src="pictures/project/rbppo_local2.gif" class='res' style="width:25%">
          </figure>
        <p class="caption">Figure: The walker gets stuck at local optima.</p>
        </center>
      </p>

      <p class="information"> 
        To deal with the problem, I proposed the concept of reward-based PPO. The idea behind this is to relax the constraint when the reward remains stable and strengthen the constrain when the reward significantly changes. With this technique, I found that using a set of hyperparameter (&alpha; = 0.10 and &tau; = 0.4) could provide good results without being stuck in local optima for all the robots tested. The locomotion of the trained robots are presented in the first video. 
        <center>
        <figure>
          <img src="pictures/project/rbppo_result.PNG" class='res'>
          </figure>
        <p class="caption">Table: Rewards obtained from the neural networks trained with KL divergence based and reward-based PPO with different hyperparameters.</p>
        </center>
      </p>
      <p class="information" style="margin-bottom: 3vw"> </p>
    </div>
    <!------------------------ <end> information row 3/2 ------------------------------------->

  <p style="margin-bottom:2%;margin-top:5%;margin-left:85%;font-size:1vw;color:#D5D5D5"> update 5 May 2022 </p>
  </div> 


  

   <div class="smallcol">
    <center> <img src="pictures/me.png" width="80%" style="margin-top: 4mm;"> </center>
    <center> 
      <b style="font-size: 1.8vw; margin-bottom: 1vw"> <span style="color: #29648A">Arthicha Srisuchinnawong</span></b>
      <p style="font-size:1.2vw;margin-top: 1vw;padding-left:2.0vw;padding-right:2.0vw;margin-bottom: 0vw"> <img src="pictures/icons/address.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : University of Southern Denmark, Odense, Denmark</p>
      <!-- <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw;margin-bottom: 1vw"> <img src="pictures/icons/tel.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : (+66) 875751549</p> -->

      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <img src="pictures/icons/gmail.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : zumoarthicha@gmail.com</p>
      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <img src="pictures/icons/outlook.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : arsri21@student.sdu.dk</p>
      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <img src="pictures/icons/line.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : arthicha_s</p>

      <p style="font-size:1.2vw;margin-top: 1vw;padding-left:1.0vw"> <a href="https://github.com/Arthicha" style="text-decoration: none; color:black;font-size:1.2vw"> <img src="pictures/icons/github.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : GitHub</a></p>
      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <a href="https://www.researchgate.net/profile/Arthicha_Srisuchinnawong" style="text-decoration: none; color:black;font-size:1.2vw"> <img src="pictures/icons/researchgate.png" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : ResearchGate</a></p>
      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <a href="https://scholar.google.com/citations?user=tllpPXsAAAAJ&hl=en&oi=ao" style="text-decoration: none; color:black;font-size:1.2vw"> <img src="pictures/icons/googlescholar.png" height="20vw" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : GoogleScholar</a></p>
      <p style="font-size:1.2vw;margin-top: 0.3vw;padding-left:1.0vw"> <a href="https://orcid.org/0000-0002-7997-745X" style="text-decoration: none; color:black;font-size:1.2vw"> <img src="pictures/icons/orcid.png" height="20vw" style="margin-top: 0vw;margin-bottom: 0vw;vertical-align:sub;height:1.2vw"> : 0000-0002-7997-745X </a></p>
    </center>
  </div>



  <script>

        var maxindex = 9;
        function clicked(main) {
          var x = document.getElementById("info"+main.toString());var y = document.getElementById("project"+main.toString());var z = document.getElementById("name"+main.toString());
          if (x.style.display === "none") {
            for (i = 1; i <= maxindex; i++) {
              var ax = document.getElementById("info"+i.toString());var ay = document.getElementById("project"+i.toString());var az = document.getElementById("name"+i.toString());
              ax.style.display = "none";ay.style.opacity = 0.3;ay.style.filter =  "blur(0.1vw)";
              az.style.opacity = 0.0;
            }
            x.style.display = "inline-block";y.style.opacity = 1.0;y.style.filter =  "blur(0.0vw)";
            z.style.opacity = 0.0;
          } else {
            x.style.display = "none";y.style.opacity = 0.3;y.style.filter =  "blur(0.1vw)";
            z.style.opacity = 0.0;
            for (i = 1; i <= maxindex; i++) {
              var az = document.getElementById("name"+i.toString());
              az.style.opacity = 1.0;
            }
          }
        }

        function hover(main) {
          var x = document.getElementById("info"+main.toString());var y = document.getElementById("project"+main.toString());var z = document.getElementById("name"+main.toString());
          if (x.style.display === "none"){
            y.style.opacity = 0.6;
            y.style.filter = "blur(0.0vw)";
          }
        }

        function over(main) {
          var x = document.getElementById("info"+main.toString());var y = document.getElementById("project"+main.toString());var z = document.getElementById("name"+main.toString());
          if (x.style.display === "none"){
            y.style.opacity = 0.3;
            y.style.filter = "blur(0.1vw)";
          }
        }
  </script>

</div>
</html>
