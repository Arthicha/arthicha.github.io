<!DOCTYPE html>
<html>
<head>  
		<title>Arthicha Srsuchinnawong</title>
		<!--
		<meta name="google-site-verification" content="5GEP9FBG2E5g1XkvGTkQ3wRMAEPauuBi_s275W4FKZk" />
		<meta http-equiv='cache-control' content='no-cache'> 
		<meta http-equiv='expires' content='0'> 
		<meta http-equiv='pragma' content='no-cache'>
		-->
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-VJ89ESJ2DZ"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-VJ89ESJ2DZ');
		</script>
		<!--
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-RFYTW3XMTK"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'G-RFYTW3XMTK');
		</script>
		-->
		<!-- Google tag (gtag.js) -->
		
</head>  

<style>
* {
	 margin: 0px;
	 padding: 0px;
}

.icon-bar {
	width: 100%;
	background-color: #AAABB8;
	overflow: auto;
}

.icon-bar a {
	float: right;
	width: 200px;
	text-align: center;
	padding: 10px 0;
	transition: all 0.3s ease;
	color: white;
	font-size: 22px;
	border-left:2px solid white;
	font-family: sans-serif;
}

.icon-bar a:hover {
	background-color: #29648A;
}

.active {
	background-color: #AAABB8; /* Green */
	color = Navy
}


.quote {
    text-align:center;
}

blockquote {
    padding:10px 5px;
    border-left:0px solid #ccc; 
    display:inline-block;
    color:Navy;
    background:#fff;
    font-size:20px;
    font-weight: bolder;
}


.wrapper {
	width: 1200px;
	margin: 0 auto;
	text-align: left;
	display: flex;
	justify-content: space-between; 
	font-family: ariel black, sans-serif;
}

.smallcol {
		width:0%;
		height:auto;
		float: left;
	 background-color:white;
	 position: -webkit-sticky;
	position: sticky;
	top: 0;
 }

.maincol {
		width:100%;
		height:auto;
		float: left;
	 background-color:white;
	 border-right: 1px solid #AAABB8;
	 border-left: 1px solid #AAABB8;
	 border-bottom: 1px solid #AAABB8;
 }

 .iconcol {
		width:400px; 
}

 .noniconcol {
		width:800px;
 }

 div.highlight{
	background:#A2D2E7;margin-left:2vw;width:94.5%;border-style: groove;border-color:#A2D2E7;
	animation: glowing 1000ms infinite;
 }

 @keyframes glowing {
	0% { background-color: #A2D2E7; box-shadow: 0 0 0px #2E9CCA; }
	50% { background-color: #A2D2E7; box-shadow: 0 0 10px #2E9CCA; }
	100% { background-color: #A2D2E7; box-shadow: 0 0 0px #2E9CCA; }
}



.list {
	word-wrap: break-word;
	text-align: justify;
	text-justify: inter-word;

	font-size:18px;
	margin-top: 0px;
	margin-left: 50px;
	padding-left:20px;
	padding-right:15px;
	margin-bottom: 0px;
	line-height: 25px;
}



p.paragraph {
	word-wrap: break-word;
	text-align: justify;
	text-justify: inter-word;

	font-size:18px;
	margin-top: 5px;
	padding-left:20px;
	padding-right:20px;
	margin-bottom: 15px;
	line-height: 25px;
}

h1.title {
	display:inline-block;
	font-size:40px;
	margin-top: 20px;
	margin-left:20px;
	margin-right:20px;
	margin-bottom: 20px;
	line-height: 20px;
	color: #16496A; 
	text-align: center;
}

img.icon {
	width:50%;
	padding-top: 0px;
	padding-bottom: 0px;
	padding-left:25%;
	padding-right:25%;
}

img.paper {
	width:90%;
	padding-top: 0px;
	padding-bottom: 0px;
	padding-left:5%;
	padding-right:5%;
}

h1.header {
	display:block;
	font-size:30px;
	margin-top: 5px;
	margin-left:20px;
	margin-right:20px;
	margin-bottom: 0px;
	line-height: 15px;
	color: #29648A;
}

h3.header {
	word-wrap: break-word;
	text-align: justify;
	text-justify: inter-word;

	display:block;
	font-size:25px;
	margin-top: 5px;
	margin-left:20px;
	margin-right:20px;
	margin-bottom: 5px;
	line-height: 32px;
	color: #29648A;
}

hr.line {
	color: #29648A;
	border: none;
	border-top: 2px solid;
}

i.year {
  color: #29648A;font-size:16px;
  font-weight: bolder;
}


p.pub {
	word-wrap: break-word;
	text-align: justify;
	text-justify: inter-word;
	font-size:1.2vw;color: black;margin-bottom:1.0vw;margin-left:0vw;margin-top:0.5vw;margin-right:2vw
}

a.link{
	text-decoration: none;
	color:  black;	
}

img.res {
	width: 60%;
	height: auto;
}

video.res {
	width: 40%;
	height: auto;
}

.caption {
	margin-top: 1.1vw;
	margin-bottom: 1.1vw;
	font-size:1.1vw;
	font-style: italic;
}

a.paperbutton {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  #f44336;
		background-color: white;
		border: 2px solid #f44336; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}

a.paperbutton:hover {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  white;
		background-color: #f44336;
		border: 2px solid #f44336; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}


a.videobutton {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  #008CBA;
		background-color: white;
		border: 2px solid #008CBA; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}

a.videobutton:hover {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  white;
		background-color: #008CBA;
		border: 2px solid #008CBA; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}

a.codebutton {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  #993d00;
		background-color: white;
		border: 2px solid #993d00; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}

a.codebutton:hover {
		-webkit-appearance: button;
		-moz-appearance: button;
		appearance: button;

		text-decoration: none;
		color:  white ;
		background-color: #993d00;
		border: 2px solid #993d00; 
		padding-left: 10px;
		padding-right: 5px;
		border-radius: 8px;
		align:  right;
}

</style>


<script>

		var images = new Array ("cover2.png","cover1.png")
		var image_count = Math.floor(Math.random() * 2);

		function rollover (image_id, millisecs) {
			var image = document.getElementById(image_id);
			image.src = "pictures/cover/"+images[image_count];
			image_count++;
			if (image_count >= images.length) {
				image_count = 0;
			}
			setTimeout("rollover('" + image_id + "'," + millisecs + ");",millisecs);
		}

		rollover("slider",1);

</script>
 
 

<body> <!-- web body -->
<div class="wrapper">  <!-- web body  wrapper-->
	<div class="maincol"> <!-- main column -->

		<div class="icon-bar"> <!-- top manu bar -->
			
			<!-- <a href="education.html" style="text-decoration: none"> <b>EDUCATION</b></a>  -->
			<!-- <a href="experience.html" style="text-decoration: none"> <b>EXPERIENCES</b></a> -->
			<a href="talk.html" style="text-decoration: none"> <b>talks</b></a> 
			<a href="publication.html" style="text-decoration: none"> <b>publication</b></a> 
			<!-- <a href="project.html" style="text-decoration: none"> <b>previous work</b></a> -->
			<a class="active" href="main.html" style="text-decoration: none"> <b>home</b> </a>
		</div>

	<!-- cover picture -->
	<img src="pictures/cover/cover0.png" id="slider" style="display:block;height:400px;width:100%;opacity:75%;"> </img>

		<br>
		<h1 class="title">
			Arthicha Srisuchinnawong
		</h1>
		<br>
	 
		
		<br>
		<h1 class='header'> Biography </h1>
		<h1 class='header'> <hr class='line' />  </h1>
		<br>

		<img src="pictures/me2.JPG" width="20%" style="padding-right: 30px; padding-left: 30px; padding-top: 15px;padding-bottom: 15px; float: right;">	

		<p class="paragraph"> 
		Arthicha Srisuchinnawong received <a class="link" href="https://www.sdu.dk/en/uddannelse/kandidat/robotteknologi" > M.Sc. in engineering â€” robot systems (advanced robotics technology) from the University of Southern Denmark (SDU), Denmark </a>, in 2023, and <a class="link" href="https://fibo.kmutt.ac.th/en/"> B.Eng. in robotics and automation engineering from the Institute of Field Robotics (FIBO), King Mongkut's University of Technology Thonburi, Thailand</a>, in 2019. Currently he is working as an academic research engineer at <a class="link" href="https://brain.vistec.ac.th/research/research-center-for-advanced-robotics-and-intelligent-automation/" > the research center for advanced robotics & intelligent automation (ARIA), Vidyasirimedhi Institute of Science and Technology (VISTEC), Thailand </a>. 
	</p>
	<p class="paragraph"> 
		 Arthicha have been collaborated with people from over 20 nationalities, published 10 peer-reviewed publications, contributed to 5 projects, and co-authored 3 patents. His work also spans across various domains, including robot learning and adaptation, interpretable modular neural control, climbing and walking robots, and exoskeleton. 
	</p>

	<p class="paragraph">
		<font> <b> He is currently persuring three challenging research questions: </b> </font>
	</p>

	<p class="paragraph">
		<font> <b> (1) Interpretable Modular Neural Control </b>: Can we understand neural control in robots? </font>
	</p>

	<p class="paragraph">
		<font> <b> (2) Closed-loop Explainable AI </b>: Can robots explain their actions, not only to us, but especially to themselves to improve performance? </font>
	</p>

	<p class="paragraph">
		<font> <b> (3) Real-world Learning </b>: Can robots continuously learn in the real world without relying heavily on simulation-based pretraining? </font>
	</p>
	<!--
	<div class="quote">
    	<blockquote>"Currently, I am seeking a Ph.D. opportunity where I can further investigate these questions. </blockquote>
    	<blockquote>Feel free to contact me at <a href="mailto:zumoarthicha@gmail.com">zumoarthicha@gmail.com</a>."</blockquote>
	</div>-->

	<!--<br> <img src="pictures/robots/zumosworks.gif" class="paper" style="margin-top: 40px"> </img> -->

	<!-- <p class="paragraph">
		 Arthicha developed the fastest leg-based pipe climbing robot (2023) and created the first tailless gecko robot climbing vertical surfaces and moving sideways to avoid obstacles (2021). These achievements have been recognized through international publications in scorpus-indexed journals and conference proceedings. 
	</p> -->
	
		<br>

	<br>
		<h1 class='header'> Education </h1>
		<h1 class='header'> <hr class='line' />  </h1>
		<br>
	<br>
	<div class="wrapper">
		<div class="iconcol"> <!-- icon column -->
			<img src="pictures/icons/sdu.png" class="icon" style="margin-top: 40px"> </img>
		</div>

		<div class="noniconcol"> <!-- non-icon column -->
			<h3 class='header'> M.Sc. Robot System (Advanced Robotics Technology)</h3>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> University of Southern Denmark (SDU), Denmark </p>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> Overall GPA 11.5 / 12.0 (96%) </p>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> <i> Sep 2021 - Jun 2023 </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Achievements </h3>
			<li class='list'> 1<sup>st</sup> place in <a href="https://www.sdu.dk/en/om-sdu/fakulteterne/teknik/nyt_fra_det_tekniske_fakultet/80-studerende-kom-med-autonome-loesninger-til-danfoss" style="text-decoration: none;"> the SDU Case Competition 2021 - Autonomous Hackathon </a>  </li>
			<li class='list'> Finalist in the SDU Case Competition 2022 - Sustainable futures.  </li>
			<li class='list'> <a href="https://www.sdu.dk/en/robotelite" style="text-decoration: none;"> Elite Summer School: Robotics and Entrepreneurship </a>   </li>
		</div>
	</div>
	<br><br><br>

	<div class="wrapper">
		<div class="iconcol"> <!-- icon column -->
			<img src="pictures/icons/kmutt.png" class="icon" style="margin-top: 80px"> </img>
		</div>

		<div class="noniconcol"> <!-- non-icon column -->
			<h3 class='header'> B.Eng. Robotics and Automation Engineering</h3>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> Institute of FIeld roBOtics (FIBO), King Mongkutâ€™s University of Technology Thonburi (KMUTT), Thailand </p>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> Overall GPA 3.87 / 4.00 (97%) </p>
			<p class='list' style="margin-left: 0px; padding-left:20px;"> <i> Jul 2015 - Jun 2019 </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Achievements </h3>
			<li class='list'> Graduated with 1<sup>st</sup> class honors & in the top 3%. </li>
			<li class='list'> <a href="https://www.kmutt.ac.th/en/education/scholarships/" style="text-decoration: none;"> Petchra Pra Jom Klao Bachelor Degree Scholarship: Academic Execellence. </a> </li>
			<li class='list'> Thailand and Institute of Field Robotics Scholarship for New Research Initiatives. </li>
			<li class='list'> Enhancing Student Mobility Scholarship. </li>
			<li class='list'> <a href="https://geo.kmutt.ac.th/%e0%b8%ab%e0%b9%89%e0%b8%ad%e0%b8%87%e0%b9%80%e0%b8%a3%e0%b8%b5%e0%b8%a2%e0%b8%99%e0%b8%97%e0%b8%b5%e0%b9%88%e0%b8%a1%e0%b8%b5%e0%b8%a3%e0%b8%b9%e0%b8%9b%e0%b9%81%e0%b8%9a%e0%b8%9a%e0%b8%81%e0%b8%b2/" style="text-decoration: none;"> Honors classes: mathematics and general physics </a> </li>
		</div>
	</div>
	
	<!--
	<br><br><br>

	<div class="wrapper">
		<div class="iconcol"> 
			<img src="pictures/icons/satit.png" class="icon"> </img>
		</div>

		<div class="noniconcol"> 
			<h3 class='header'> High School Diploma (with focus in mathematics and sciences) </h3>
			<p class='list'> Chiang Mai University Demonstration School, Chiang Mai, Thailand </p>
			<p class='list'> Overall GPA 3.91 / 4.00 (98%) </p>
			<p class='list'> <i> May 2012 - Mar 2015 </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Achievements </h3>
			<li class='list'> 7<sup>th</sup> place in the QUOTA examination score <i class="year"> (2015) </i> </li>
			<li class='list'> Full score in the mathematics examination of the Ordinary National Educational Test (O-NET). <i class="year"> (2015) </i> </li>
		</div>
	</div> -->


	<br><br><br>

	<br>
		<h1 class='header'> Selected Publications </h1>
		<h1 class='header'> <hr class='line' />  </h1>
		<br>
	<br>

	<div class="wrapper" >
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/GOLLUM.gif" class="paper" style="margin-top: 30px"> </img>
			<img src="pictures/publication/GOLLUMterrains.gif" class="paper" style="margin-top: 0px"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'>  Growable and Interpretable Neural Control with Online Continual Learning for Autonomous Lifelong Locomotion Learning Machines </h3>
			<h3 class='header' style="font-size:16px"> International Journal of Robotics Research (IJRR), 2025 </h3>
			<p class='paragraph'> This work presents Growable Online Locomotion Learning under Multicondition (GOLLUM) for continual locomotion learning, addressing challenges such as interpretability, inefficiency, knowledge exploitation, and catastrophic forgetting. GOLLUM leverages interpretability through layer-wise neural control encoding and column-wise robot skill encoding (ring-like structures), enabling interpretable skill acquisition and retention. On a physical hexapod robot, GOLLUM autonomously acquires and retains diverse locomotion skills from scratch, without simulation or human intervention, contributing to lifelong robot learning and adaptation. Additionally, GOLLUM advances robot-inspired bio models, bio-inspired control, and engineering solutions such as multimodal learning by demonstration and self-organized behavior hierarchies, emphasizing its broader potential.</p>
			<br>
			<p align='right'>
				 <a href="https://youtu.be/PxAl___xCT8" class="videobutton"> Video </a> &nbsp; <a href="https://youtu.be/mgONmN1hBwo" class="videobutton"> Video </a> &nbsp;
			</p>
		</div>
		</a> 
	</div>

	<br>

	<div class="wrapper" >
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/loft.gif" class="paper" style="margin-top: 40px"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'>  Unsupervised Multiple Proactive Behavior Learning of Mobile Robots for Smooth and Safe Navigation </h3>
			<h3 class='header' style="font-size:16px"> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024 </h3>
			<p class='paragraph'> This study introduces a model-free neural control architecture featuring a plug-and-play Multiple Proactive Behavior Learning (MPL) module. Unlike existing approaches that rely on model-based assumptions and extensive data, the MPL adapts control policies online in an unsupervised manner using minimal data. By correlating sensory inputs with local planner commands, the MPL enables autonomous learning of proactive behaviors for smoother motion and collision avoidance. It improves motion smoothness by 10% and reduces collisions by 30% in static environments, and cuts collisions by up to 70% in dynamic settings, offering an efficient cooperation betwen model-free and model-based controls.</p>
			<br>
			<p align='right'>
				 <a href="https://ieeexplore.ieee.org/abstract/document/10802071/" class="paperbutton"> Paper </a>  &nbsp; <a href="https://youtu.be/yF7n7kNFumo" class="videobutton"> Video </a> &nbsp; <a href="https://github.com/Arthicha/Multiple-Proactive-Behavior-Learning.git" class="codebutton"> Code </a> &nbsp;
			</p>
		</div>
		</a> 
	</div>

	<br>

	<div class="wrapper">
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/avis.gif" class="paper" style="margin-top: 80px"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'> Adaptive Bipedal Robot Walking on Industrial Pipes under Neural Multimodal Locomotion Control: Toward Robotic Out-pipe Inspection </h3>
			<h3 class='header' style="font-size:16px">  IEEE/ASME Transactions on Mechatronics (TMECH), 2023 </h3>
			<p class='paragraph'> Out-pipe inspection robots face challenges in balancing on curved surfaces, climbing pipes, overcoming obstacles, and transitioning between segments. This work introduces an adaptive bipedal robot with neural multimodal locomotion control for semi-autonomous inspection. Using eight interpretable neural modules, the control system enables the robot to adapt to various locomotion modes, achieving a speed of 10 cm/s on both horizontal and vertical pipes, improving energy efficiency by over 200% compared to previously developed robots, and overcoming obstacles up to 14 cm in height.</p>
			<br>
			<p align='right'>
				<a href="https://ieeexplore.ieee.org/abstract/document/10190136" class="paperbutton"> Paper </a>  &nbsp;   <a href="https://www.youtube.com/embed/SpwumbieLr8" class="videobutton"> Video </a> &nbsp;
			</p>
		</div>
	</div>

	<br>

	<div class="wrapper">
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/exo_amnc.gif" class="paper" style="margin-top: 80px"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'> Adaptive Modular Neural Control for Online Gait Synchronization and Adaptation of an Assistive Lower-Limb Exoskeleton </h3>
			<h3 class='header' style="font-size:16px">  IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2023 </h3>
			<p class='paragraph'> This study presents an adaptive modular neural control (AMNC) system for online gait synchronization of a lower-limb assistive exoskeleton, aimed at preventing conflicting movements and enhancing assistance performance. The AMNC leverages distributed neural modules that interact to utilize neural dynamics and feedback signals, enabling real-time synchronization of exoskeleton movements with the user's gait while reducing tracking errors. Compared to existing methods, AMNC enhances phase, frequency, and shape adaptation, achieving up to an 80% reduction in tracking errors and a 30% decrease in unseen interaction torque.</p>
			<br>
			<p align='right'>
				<a href="https://ieeexplore.ieee.org/document/10097588" class="paperbutton"> Paper </a>  &nbsp;   <a href="https://youtu.be/WwdrFzYO8fY" class="videobutton"> Video </a> &nbsp; <a href="https://github.com/Arthicha/EXOVIS_AMNC" class="codebutton"> Code </a> &nbsp;
			</p>
		</div>
	</div>

	<br>

	<div class="wrapper">
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/neurovis.gif" class="paper" style="margin-top: 100px"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'> NeuroVis: Real-time Neural Information Visualization of Embodied Neural Systems </h3>
			<h3 class='header' style="font-size:16px">  Frontiers in Neural Circuits, 2021 </h3>
			<p class='paragraph'> The study introduces "NeuroVis", a novel tool for real-time measurement and visualization of neural spatial-temporal information. NeuroVis visualizes neural structure, neural dynamics, neural plasticity, and neural memory, by using spatial positions, color gradients, connection thickness, and luminous intensity changes. The study presents three use cases: function approximation, robot locomotion control and learning, and large-scale adaptive robot locomotion control, demonstrating how NeuroVis effectively tracks and analyzes these neural components in real-time. The tool aims to improve the understanding of embodied dynamic neural information processes, boost efficient neural technology development, and enhance user trust. </p>
			<br>
			<p align='right'>
				<a href="https://www.frontiersin.org/articles/10.3389/fncir.2021.743101/full" class="paperbutton"> Paper </a> &nbsp; <a href="https://youtu.be/-GHfUVBz1ek?si=bVjrWLihngqLcRgS" class="videobutton"> Video1 </a> &nbsp; <a href="https://youtu.be/D29m6il3Mp0?si=52LWUKu9T98ZJd-G" class="videobutton"> Video2 </a> &nbsp; <a href="https://gitlab.com/zumoarthicha/neurovis" class="codebutton"> Code </a> &nbsp; 
			</p>
		</div>
	</div>

	<br>

	<div class="wrapper">
		<div class="iconcol" onclick="location.href='publication.html'"> <!-- icon column -->
			<img src="pictures/publication/geckorobot.gif" class="paper" style="margin-top: 20px; width:50%; padding-left:25%;padding-right:25%"> </img>
		</div>

		<div class="noniconcol" onclick="location.href='publication.html'"> <!-- non-icon column -->
			<h3 class='header'> Modular Neural Control for Gait Adaptation and Obstacle Avoidance of a Tailless Gecko Robot </h3>
			<h3 class='header' style="font-size:16px">  Journal of Intelligent and Robotic Systems (JINT), 2021 </h3>
			<p class='paragraph'> This study presents a neural control architecture for gait adaptation and obstacle avoidance in a tailless gecko robot. The system employs a three-layer hierarchical structure: sensory preprocessing, central pattern generation (CPG), and CPG postprocessing. Sensory modules filter noise and initiate behaviors, while the CPG produces rhythmic patterns for different gaits (wave, intermediate, trot) and climbing directions (forward, sideways). The robot adapts its gait on slopes, including 90Â°, using body inclination sensors and detects obstacles with infrared sensors, switching from forward to sideways climbing. </p>
			<br>
			<p align='right'>
				<a href="https://doi.org/10.1007/s10846-020-01285-y" class="paperbutton"> Paper </a>  &nbsp;   <a href="https://youtu.be/Uq8vQsu0m98" class="videobutton"> Video </a> &nbsp;
			</p>
		</div>
	</div>
	
	

	<br><br><br>

	
		
	<!--

	<br>
		<h1 class='header'> Experience </h1>
		<h1 class='header'> <hr class='line' />  </h1>
		<br>
	<br>

	<div class="wrapper">
		<div class="iconcol"> 
			<img src="pictures/icons/brain.png" class="icon" style="margin-top: 80px"> </img>
		</div>

		<div class="noniconcol"> 
			<h3 class='header'> Research Engineer </h3>
			<p class='list'> Bio-Inspired Robotics & Neurl Engineering Laboratory (BRAIN-LAB), VISTEC, Thailand </p>
			<p class='list'> <i> Jul 2020 - now </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Tasks </h3>
			<li class='list'> Advanced Hybrid Legged-Wheeled Robot for Internal Vessal Inspection <i class="year"> (2021-2022) </i> </li>
			<li class='list'> Adaptive Modular Neural Control for Online Gait Synchronization and Adaptation of an Assistive Lower-Limb Exoskeleton <i class="year"> (2021) </i> </li>
			<li class='list'> Advanced Pipe Inspection Robot System <i class="year"> (2020-2021) </i> </li>
			<li class='list'> Real-time neuro visualization for undestanding and analysing neural robot control <i class="year"> (2020-2021) </i> </li>
		</div>
	</div>		
	
	<br><br><br>

	<div class="wrapper">
		<div class="iconcol"> 
			<img src="pictures/icons/ens.png" class="icon" style="margin-top: 20px"> </img>
		</div>

		<div class="noniconcol"> 
			<h3 class='header'> Student Assistance</h3>
			<p class='list'> Embodied-AI & Neurorobotics Laboratory (ENS-LAB), SDU, Denmark </p>
			<p class='list'> <i> Feb 2022 - Dec 2022 </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Tasks </h3>
			<li class='list'> Long-term autOnomy For service robots in consTruction (LOFT) <i class="year"> (2022) </i> </li>
		</div>
	</div>
	
	<br><br><br>

	
	<div class="wrapper">
		<div class="iconcol"> 
			<img src="pictures/icons/neutron.png" class="icon" style="margin-top: 50px"> </img>
		</div>

		<div class="noniconcol"> 
			<h3 class='header'> Research Intern</h3>
			<p class='list'> Neurorobotic Technology for Advanced Robot Motor Control (NEUTRON Lab), NUAA, China </p>
			<p class='list'> <i> May 2018 - Jan 2019 </i></p>
			<br>
			<h3 class='header' style="font-size:18px"> Tasks </h3>
			<li class='list'> Neural locomotion control for gait adaptation and obstacle avoidance of tailless gecko-inspired climbing robots. <i class="year"> (2018-2019) </i> </li>
		</div>
	</div>  -->


	<br>
		<h1 class='header'> Contact </h1>
		<h1 class='header'> <hr class='line' />  </h1>
	<br>

	<p class="list"> <img src="pictures/icons/address.png" style="vertical-align:sub;height:22px"> : Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand </p>
	<p class="list"> <img src="pictures/icons/mail.png" style="vertical-align:sub;height:22px"> : zumoarthicha@gmail.com, arthichas_pro@vistec.ac.th</p>
	<p class="list"> <a href="https://github.com/Arthicha" style="text-decoration: none;"> <img src="pictures/icons/github.png" style="height:22px"> : Arthicha</a></p>
	<p class="list"> <a href="https://scholar.google.com/citations?user=tllpPXsAAAAJ&hl=en&oi=ao" style="text-decoration: none;"> <img src="pictures/icons/googlescholar.png" style="height:22px"> : Arthicha Srisuchinnawong</a></p>
	<p class="list"> <a href="https://orcid.org/0000-0002-7997-745X" style="text-decoration: none"> <img src="pictures/icons/orcid.png" height="20vw" style="height:22px"> : 0000-0002-7997-745X </a></p>



	<p style="float: right;margin-bottom:20px;margin-top:20px;margin-right:20px;font-size:18px;color:#D5D5D5"> update 8 February 2025 </p>
	
	</div> <!-- END main column -->


</div> <!-- END body wrapper -->
</body>
</html>
